\documentclass[10pt]{article}

%% Various useful packages and commands from different sources

\usepackage[applemac]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{cite, url,color} % Citation numbers being automatically sorted and properly "compressed/ranged".
%\usepackage{pgfplots}
\usepackage{graphics,amsfonts}
\usepackage[pdftex]{graphicx}
\usepackage[cmex10]{amsmath}
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
 \interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally does.

% Compact lists
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{fancyvrb}

\usepackage{listings} % for Matlab code
\definecolor{commenti}{rgb}{0.13,0.55,0.13}
\definecolor{stringhe}{rgb}{0.63,0.125,0.94}
\lstloadlanguages{Matlab}
\lstset{% general command to set parameter(s)
framexleftmargin=0mm,
frame=single,
keywordstyle = \color{blue},% blue keywords
identifierstyle =, % nothing happens
commentstyle = \color{commenti}, % comments
stringstyle = \ttfamily \color{stringhe}, % typewriter type for strings
showstringspaces = false, % no special string spaces
emph = {for, if, then, else, end},
emphstyle = \color{blue},
firstnumber = 1,
numbers =right, %  show number_line
numberstyle = \tiny, % style of number_line
stepnumber = 5, % one number_line after stepnumber
numbersep = 5pt,
language = {Matlab},
extendedchars = true,
breaklines = true,
breakautoindent = true,
breakindent = 30pt,
basicstyle=\footnotesize\ttfamily
}

\usepackage{array}
% http://www.ctan.org/tex-archive/macros/latex/required/tools/
\usepackage{mdwmath}
\usepackage{mdwtab}
%mdwtab.sty	-- A complete ground-up rewrite of LaTeX's `tabular' and  `array' environments.  Has lots of advantages over
%		   the standard version, and over the version in `array.sty'.
% *** SUBFIGURE PACKAGES ***
\usepackage[tight,footnotesize]{subfigure}
\usepackage[top=2cm, bottom=2cm, right=1.6cm,left=1.6cm]{geometry}
\usepackage{indentfirst}


\setlength\parindent{0pt}
\linespread{1}

\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\begin{document}
\title{Digital Transmission - Homework 1}
\author{Andrea Dittadi, Davide Magrin, Michele Polese}

\maketitle

\section{Estimating the PSD}
% some lines on what is PSD?
In order to estimate the PSD of a given a process $z(k)$ it is necessary to consider two hypothesis. The first is that the process is at least wide sense stationary. A process $z(k)$ is WSS if $m_z(t) = m_z, \forall t$ and $r_z(t, t - \tau) = r_z(\tau), \forall t$. By inspecting the mean and autocorrelation over different non-overlapping windows of 100 samples it appears that the mean is quite constant, as it is the estimate of the real part of the autocorrelation, while the imaginary part of the autocorrelation changes more than the two other estimates. Instead, by considering the sample mean of different subsequences of 20 samples inside each bigger window the real and imaginary parts of the mean don't vary too much, despite the great variance that an estimator with few samples has. We conclude that in each window of less than 100 sample the process can be considered WSS, while it is harder to state that it is WSS in all the 1000 samples.
% Because of this consideration we will use windows of 100 samples to analyze the PSD of the process and derive conclusions on the presence of the spectral lines.

% Do we agree on this Does it make sense? Can we provide some figures?
% Or we skip these considerations and say that it is wss?

The second hypothesis is that the process is ergodic, and as suggested in \cite{bc} we assume that every WSS process is also ergodic. In this way from a single representation of the process it is possible to estimate the autocorrelation and thus the PSD of the whole process. This is a key assumption for the analysis of the process we are given, without this hypothesis not much can be said. \\
We also subtract the sample mean by the given signal $z(k)$, since the continuous component doesn't bring any relevant information to the estimates of the PSD of the process. Thus the signal to which we will refer in the following spectral analysis is a zero-mean signal with the form $z(k) = z(k) - \frac{1}{K} \sum_{i=1}^K z(k)$ with $K$ the number of samples of the signal. \\
In order to estimate the PSD of the process and find the spectral lines we used a combination different estimators that will be introduced in the following sections. \\
Note that unless specified we assume a unitary sampling period $T_c = 1$.

\subsubsection*{Periodogram}
The periodogram is an estimator of the PSD based of the Fourier transform of the signal:
\begin{equation}
  \mathcal{P}_{PER} = \frac{1}{K T_c} |\mathcal{\tilde{Z}}(f)|^2
\end{equation}
The Fourier transform $|\mathcal{\tilde{Z}}(f)|$ is computed as a DFT with the function \verb fft  provided in MATLAB. This is a biased estimator, because of the fact that the signal is windowed over a finite number of samples. In order to improve the quality of this estimate and to detect more precisely the spectral lines it is possible to apply a different window than the rectangular one. In particular in some cases we applied a Kaiser window (from \cite{mitra}) to the signal before the computation the \verb fft. This is a tunable window in the form
\begin{equation}
  w(k) = \frac{I_0 \bigg[w_a \sqrt{1-\big(1-\frac{2(n+1)}{L+1}\big)^2}\bigg]}{I_0[w_a]} \; \; \; \; \; \;  0 \le n < L
\end{equation}
with $I_0[x]$ the zero-order Bessel function and $w_a$ a tunable parameter that depends on the attenuation of the secondary lobes of the window. In this way it is possible to reduce spectral leakage and make real spectral lines stand out among the peaks produced by the noise. For an attenuation $A_{dB} > 50$ dB then $w_a = 0.1102(A_{dB} - 8.7)$. Given the length and the parameter $w_a$ the Kaiser window can be computed using MATLAB function \verb kaiser .
% Alternatively, $w_a = 5.65$ -> 60 dB %

\subsubsection*{Welch periodogram}
The idea behind Welch periodogram is to compute the periodograms on different windows of the given signal and averaging them for each frequency. Each subsequence $z^{(s)}$ is composed of $D$ consecutive samples of the original signal, and has $S$ samples in common with the previous and the following windows. Thus given the length $K$ of the original signal the number of subsequences is $N_s = \floor{\frac{K - D}{D - S} + 1}$. For each subsequence $z^{(s)} = w(k)z(k + s(D-S)), k=0,1,..., D- 1, s = 0, 1, ... N_s - 1$, with $w(k)$ a window of $D$ samples and normalized energy $M_w = \frac{1}{D}\sum_{k=0}^{D-1}w^2(k)$, we compute the periodogram $\mathcal{P}_{PER}^{(s)}(f) = \frac{1}{D T_c M_w} |\mathcal{\tilde{Z}}^{(s)}(f)|^2$.
Therefore the Welch estimate is given by
\begin{equation}
  \mathcal{P}_{WE}(f) = \frac{1}{N_s}\sum_{s=0}^{N_s-1}{P}_{PER}^{(s)}(f)
\end{equation}

\subsubsection*{Unbiased estimate of autocorrelation}
The autocorrelation is estimated using
\begin{equation}
  \hat{r}_z(n) = \frac{1}{K - n}\sum_{k=n}^{K-1} z(k) z^*(k-n)
\end{equation}
with $n = 0, 1, ... K/5$ in order to avoid poor estimates given by the fact that as $n$ approaches $K$ the number of samples used in the estimate becomes smaller and smaller, and the variance of the estimator increases.

\subsubsection*{Blackman and Tukey correlogram}
This estimator uses an unbiased autocorrelation sequence estimate ${\hat{r}_z(n)}, n = -L, ..., L$ and computes the Fourier transform of the ACS multiplied by a window $w(k)$ of length $2*L + 1$ and $w(0)=1$ (both requirement are satisfied by the Kaiser window of the previous section):
\begin{equation}
  \mathcal{P}_{CORR}(f) = T_c\sum_{n=-L}^{L}w(n)\hat{r}_z(n)e^{-j2\pi f n T_c}
\end{equation}
Note that $L \le K/5$ because of the considerations about the variance of ACS estimate. In particular we chose $L = K/5$. Moreover the ACS estimator of the previous section is computed for non-negative values of $n$, so the samples for $n =-1, -2, ..., -L$ are computed exploiting the hermitian symmetry of the ACS ($\hat{r}_z(-n) = \hat{r}_z^*(n)$). \\

\subsubsection*{AR model}
The last estimate of the PSD can be computed using an AR model of order N to describe the process. This model considers the process as generated by a recursive equation
\begin{equation}
  z(k) = - \sum_{n=1}^N a_n z(k-n) + w(k)
\end{equation}
with $w$ white noise with variance $\sigma_w^2$. This equation is equivalent to a scheme where $w$ is filtered by a filter with transfer function $H_{AR}(z) = 1/A(z)$ with $A(z) = 1 - \sum_{n=1}^N a_n z^{-n}$. Then the z-transform of the ACS is given by $P_z(z) = \frac{\sigma_w^2}{A(z)A^*(\frac{1}{z^*})}$, therefore the PSD, which is the Fourier transform of the ACS, is
\begin{equation}
  \mathcal{P}_{AR} = P_z(e^{j 2 \pi f T_c}) = \frac{\sigma_w^2}{|\mathcal{A}(e^{j 2 \pi f T_c})|^2} = \frac{\sigma_w^2}{|\mathcal{A}(f)|^2}
\end{equation}
The coefficients $a_1, a_2, ..., a_N$ can be computed using the Yule-Walker equation. Given the autocorrelation matrix
\begin{equation}
  \mathbf{R} =
  \begin{bmatrix}
     r_z(0) &  r_z(-1) & \cdots &  r_z(-N+1) \\
     r_z(1) &  r_z(0)  & \cdots &  r_z(-N+2) \\
    \vdots       & \vdots        & \vdots & \vdots \\
     r_z(N-1) &  r_z(N-2) & \cdots &  r_z(0)
  \end{bmatrix}
\end{equation}
and $ \mathbf{r} = [r_z(1),  r_z(2),  r_z(N)]^T $ the vector $\mathbf{a}$ of the coefficients of the AR model is given by
\begin{equation}
  \mathbf{R} \, \mathbf{a} = - \mathbf{r}
\end{equation}
and the variance of the white noise driving the model is
\begin{equation}
  \sigma_w^2 = \hat{r}_z(0) + \mathbf{r}^H\mathbf{a}
\end{equation}
Since the complete statistical description of the process isn't available, we compute the autocorrelation matrix $\mathbf{R}$ using the ACS estimate described previously. Therefore $\mathbf{R}_{(i, j)} = \hat{r}_z(i - j)$, where the samples for $n = i - j=-1, -2, ..., -N$ are computed using the hermitian symmetry of the ACS. \\


\subsubsection*{Identification of spectral lines}
The identification of spectral lines is usually carried out by inspection of the PSD of the process. In this problem there are two factors that make this procedure unfeasible: the low signal to noise ratio and the length of the signal we are given. Moreover the hypothesis of wide sense stationarity should be considered carefully, since we don't know the nature of the underlying process nor if the 1000 samples are sampled with a high period or not. \\
The low amplitude of the spectral lines, compared to the power of the noise, doesn't allow to clearly distinguish their presence by inspecting the PSD estimated in different ways over the whole 1000 samples. There are some peaks but it isn't straightforward to conclude that they represent noise peaks or spectral lines. On the other side, there's some literature on detection of sinusoidal components with low amplitude in wideband noise, such as the article in \cite{lowamp}, but the algorithms proposed requires more than 1000 samples.\\
This can be seen in Figure~\ref{fig:complete_psd} where different estimates of the PSD are plotted.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{images/psd.pdf}
  \caption{Different spectral estimates}
  \label{fig:complete_psd}
\end{figure}
The parameters of the estimators are as follows. For the periodogram the window used is the default rectangular one. For Welch periodogram $D = 200, S = D/2$ and the window chosen is a Kaiser window with sidelobe attenuation of 60 dB ($w_a = 5.65$). The order of the autocorrelation estimator for the correlogram is $N_{corr} = K/5 = 200$, the window applied is once again a Kaiser window with 60 dB sidelobe attenuation. The AR model has order 3, since the knee of the noise variance $\sigma_w^2$ is present at $N=3$ as it can be seen in Figure~\ref{fig:complete_knee}.

\begin{figure}
  \centering
  \includegraphics[width = 0.7\textwidth]{images/complete_knee.pdf}
  \caption{Variance of AR model white noise as a function of the order N}
  \label{fig:complete_knee}
\end{figure}

In order to identify which peaks in Figure~\ref{fig:complete_psd} belong to the noise and which are given by low amplitude spectral lines we developed an algorithm that tries to tackle the issues described before and produce some clear results. As first we consider different windows of samples, of length $L$ and with the initial samples separated by $S$ samples. They overlap by $\max\{0, L-S\}$ samples. The number of windows is $N = \floor{(K-L)/S}$ with $K$ the length of the whole signal. For each of these short windows we compute the periodogram and find its peaks. We keep a running sum of the peaks found at each frequency (note: they are quantized by the length $L$ of the window chosen, and the eventual zero padding of the fft) and increase it by 1 whenever there's a peak. In this way we artificially observe more than one realization of the process, and if a peak in the periodogram is due to the noise present at a certain instant it won't be present in the next windows. On the other hand, if a peak is due to a weak sinusoidal signal then it will be present in more than one window and the sum of peaks found at that frequency will be higher than the ones at frequencies with just the noise. We chose to consider the periodogram and not the other estimators because the length $L$ of the windows we considered is always below or equal 100 samples, otherwise the number of different windows $N$ would be too low. The Welch and correlogram estimator are less effective with so few samples. For the first there's a necessary trade-off between the number of windows $N_s$ and their length $D$: the number of samples in each window should be enough to represent the process, but the littler $N_s$ is the more the Welch periodogram becomes similar to a simple periodogram with high variance. Also the Blackman and Tukey correlogram suffers in the presence of short windows because it transforms the estimate of the ACS which is reliable only if computed on less than a fifth of the window's samples.\\
In Figure~\ref{fig:peak_acc} there's the result of an iteration of the algorithm on the given signal, with parameters $S = 64, L = 96$ and a periodogram computed by windowing the signal with a Kaiser window with parameter $w_a = 5.65$ which guarantees a sidelobe attentation of 60 dB, without increasing too much the size of the main lobe (this allows to reduce spectral leakage without increasing too much the smearing).

\begin{figure}[h!]
  \centering
  \includegraphics[width = 0.7\textwidth]{images/period_acc_temp.pdf}
  \caption{Accumulation of peaks in the signal}
  \label{fig:peak_acc}
\end{figure}

In can be clearly seen that there is a peak for $f_0 = 0.774$ in about 90\% of the windows, while the other peaks are much lower. Thus we conclude the the only spectral line present in the signal has the form $A e^{j 2 \pi f_0 n T_c}$. This peak is present also in~\ref{fig:complete_psd} and periodogram, Welch and correlogram represent it. The AR model, instead, doesn't change its behaviour in the presence of the peak, probably because the amplitude of the complex sinusoid is too low.


\section{AR models for continuous and spectral lines components}

\section{Amplitude, phase and frequency estimate of the spectral line}

\section{LMS predictor}
The setting of the predictor problem is as follows. Given a signal $z(k)$, WSS, with zero mean, let $\mathbf{z}^T(k-1)$ be the vector $[z(k-1), z(k-2), ..., z(k-N)]$. The one-step predictor of order N tries to estimate the value of $z(k)$ given $\mathbf{z}^T(k-1)$. This problem can be solved considering $\mathbf{z}^T(k-1)$ the input of a Wiener filter of order N and $z(k)$ the reference signal. Then the Wiener-Hopf equation computes the optimal coefficients of the filter with $\mathbf{R}_N \mathbf{c}_{opt} = \mathbf{r}_N$, given the autocorrelation matrix $\mathbf{R}_N$ and $\mathbf{r}_N = [r_z(1), r_z(2), ..., r_z(N)]^T$. \\
Both \textit{least mean-square} (LMS) and \textit{recursive least square} (RLS) algorithms provide a recursive method to compute an approximation of the optimal Wiener-Hopf solution for the predictor problem without using the autocorrelation matrix $\mathbf{R}_N$ and the vector $\mathbf{r}_N$.
In particular LMS is a version of the gradient algorithm that updates at each iteration $k$ the coefficients of the Wiener filter with the equation
\begin{equation}
  \mathbf{c}(k+1) = \mathbf{c}(k) + \mu e(k) \mathbf{x}^*(k-1)
  \label{eq:lms_up}
\end{equation}
The implementation of LMS is simple and it doesn't require too much computation. There are two parameters, the order $N$ and the update coefficient $\mu = \frac{\tilde{\mu}}{N r_z(0)}$, and it must hold $0 < \tilde{\mu} < 2$ for convergency. Then, at each step, compute:
\begin{equation}
  y(k) = \mathbf{z}^T(k-1) \mathbf{c}(k)
\end{equation}
\begin{equation}
  e(k) = z(k) - y(k)
\end{equation}
and update coefficients with Equation~\ref{eq:lms_up}. At the beginning $\mathbf{c}(0) = \mathbf{0}$, and $z(k) = 0, \; \forall \; k < 0$. \\

We applied the LMS algorithm to the continuous part of the signal as filtered in Section 2. Given the AR model of order N for the continuous part of the signal, the $\mathbf{c}$ vector should converge to $-\mathbf{a}$ and $|e(k)|^2 = \sigma_w^2$ at least in mean.




\section{RLS algorithm}

\begin{thebibliography}{10}

\bibitem{bc}
Benvenuto, Cherubini, Algorithms for Communications Systems and their Applications, Wiley, 2004

\bibitem{mitra}
Mitra, Digital Signal Processing: A Computer-based Approach, McGraw-Hill international, 2001

\bibitem{lowamp}
Lee, Tang, Chan, Detecting weak sinusoidal signals embedded in a non-stationary random broadband noise, Journal of Sound and Vibration, 2007, N. 304, pages 831 - 844

\end{thebibliography}

\end{document}
