\documentclass[10pt]{article}

%% Various useful packages and commands from different sources

\usepackage[applemac]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{cite, url,color} % Citation numbers being automatically sorted and properly "compressed/ranged".
%\usepackage{pgfplots}
\usepackage{graphics,amsfonts}
\usepackage[pdftex]{graphicx}
\usepackage[cmex10]{amsmath}
\usepackage{bm}
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
 \interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally does.

% Compact lists
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{fancyvrb}

\usepackage{listings} % for Matlab code
\definecolor{commenti}{rgb}{0.13,0.55,0.13}
\definecolor{stringhe}{rgb}{0.63,0.125,0.94}
\lstloadlanguages{Matlab}
\lstset{% general command to set parameter(s)
framexleftmargin=0mm,
frame=single,
keywordstyle = \color{blue},% blue keywords
identifierstyle =, % nothing happens
commentstyle = \color{commenti}, % comments
stringstyle = \ttfamily \color{stringhe}, % typewriter type for strings
showstringspaces = false, % no special string spaces
emph = {for, if, then, else, end},
emphstyle = \color{blue},
firstnumber = 1,
numbers =right, %  show number_line
numberstyle = \tiny, % style of number_line
stepnumber = 5, % one number_line after stepnumber
numbersep = 5pt,
language = {Matlab},
extendedchars = true,
breaklines = true,
breakautoindent = true,
breakindent = 30pt,
basicstyle=\footnotesize\ttfamily
}

\usepackage{array}
% http://www.ctan.org/tex-archive/macros/latex/required/tools/
\usepackage{mdwmath}
\usepackage{mdwtab}
%mdwtab.sty	-- A complete ground-up rewrite of LaTeX's `tabular' and  `array' environments.  Has lots of advantages over
%		   the standard version, and over the version in `array.sty'.
% *** SUBFIGURE PACKAGES ***
\usepackage[tight,footnotesize]{subfigure}
\usepackage[top=2.2cm, bottom=2.2cm, right=1.7cm,left=1.7cm]{geometry}
\usepackage{indentfirst}


%\setlength\parindent{0pt}
\linespread{1}

\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\M} {\mathtt{M}}
\newcommand{\dB} {\mathrm{dB}}
\newcommand{\tr} {\mathrm{tr}}
\newcommand{\lmod}[1] {_{\,\mathrm{mod}\,#1}}



\graphicspath{ {figures/} }

% equations are numbered section by section
%\numberwithin{equation}{section}


\begin{document}
\title{Digital Transmission - Homework 3}
\author{Andrea Dittadi, Davide Magrin, Michele Polese}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% PROBLEM 1 %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Problem 1}

% Spiegare cosa mandiamo e come lo generiamo
In this problem the channel $q(n\frac{T}{4})$ is given at $T_c = \frac{T}{4}$. The goal is to estimate at the receiver the timing phase $t_0$ and the equivalent channel impulse response in $T$ with LS method.
The setting of the system is as in Fig.~\ref{fig:channel_T4}. In order to perform LS estimation the first $L + N_{seq} = 15 + 10$ symbols of the sequence $a_k$ are binary symbols out of a QPSK constellation with $\sigma_a^2 = 2$. 
In particular they are $1+j$ and $-1-j$ and map the $+1$ and $-1$ values of a ML sequence of length $L = 15$, partially repeated by $N_{seq}=10$. Then QPSK data follows. 
The conversion between a bit stream and QPSK is carried out by the \texttt{bitmap} function. %% TODO some more on this?? Give a better formulation

\begin{figure}
	\centering
	\includegraphics[width = 0.5\textwidth]{channel_T4}
	\caption{System setting for problem 1}
	\label{fig:channel_T4}
\end{figure}

%%TODO Does Eq go in dB???? 

% Spiegare che channel_output funziona da polifase
The noisy output $r_n$ of the channel is computed with a polyphase version of the channel impulse response $q(nT_c)$. Each branch is $q^{i}(mT) = q(mT + i)$ with $i \in [0, 3]$ , and since it is not time-varying it is possible to directly compute the convolution of each branch and the $a_k$ sequence. At the output of each convolution white noise $w_n$ is added, with variance $\sigma_w^2 = \frac{\sigma_a^2 E_q}{4 \Gamma} = -19.2411$ dB, where $\sigma_a^2 = 2$, $E_q = 2.3819$ and $\Gamma = 20$ dB is the given SNR at receiver input. Then a parallel to series converter generates the output at time $T_c$.

% !!! Spiegare come stimiamo t0, riportare l'equazione (Formula 7.269, channel_estimator)
The first operation performed by the receiver is the estimation of $t_0$ as optimal timing phase in $T_c$. In particular, given the stream of symbols $r_n$, and the knowledge of the ML sequence $a_l, l\in [0, L-1]$, the receiver computes

\begin{equation}
	m_{opt} = \argmax_{m} | x_{ra}(mT_c) | = \argmax_{m} \left| \frac{1}{L} \sum_{l = 0}^{L - 1} r_{(lT + mT_c)}a^*_l \right|
\end{equation}
for $ 0 < m < 40$ and finds that
\begin{equation}
	m_{opt} = 10
\end{equation}
\begin{equation}
	t_{0} = m_{opt}T_c = 10 T_c.
\end{equation}

In the domain with sampling period $T_c$, the timing phase is $t_0 = 10$, and we define the corresponding channel delay $D_{ch} = ({t_0}\lmod{4})T = 2T$ as the number of samples after which the receiver receives the first sample of the training sequence multiplied by $h_0$ -- which is the coefficient of the channel impulse response with the greatest amplitude -- plus some ISI in case there are some precursors. In other words, if the receiver and the transmitter use the same time index $k$ and if $x_k$ is the output at time $T$ of the receiver then
\begin{equation}
x_{k} = h_0a_{k-D_{ch}} + i_k, \quad i_k = \sum_{\substack{i = -N_1 \\ i \neq 0}}^{N_2} h_i a_{k-D_{ch}-i}
\end{equation}
where $i_k$ is the inter-symbol interference. Therefore, if $t_0 = 10T_c$ and $D_{ch} = 2T$ there could be at most 2 precursors (i.e. the coefficients of $h_i$ for $i = -N_1, \dots, -1$), and they can be equal to 0 (they don't introduce ISI but just the delay $D_{ch}$) or different from 0 but with an amplitude lower than $h_0$. 

The receiver can now perform the estimate of the channel impulse response at time $T$. The output of the channel $r_n$ is sampled at $n = t_0 + kT$ to get $x_k = r_{t_0 + kT}$. The LS estimate is computed using the symbols of the known sent ML sequence $a_k$, of length $L + N_{seq} = 25$, and a suitable section of the output. Note that the training sequence has $N_{seq} = 10$, enough to avoid the transient of the channel -- which is $N - 1$ samples long, with $N = N_1 + N_2 + 1$ the actual length of the impulse response.

Note also that the LS method can estimate a causal impulse response, $\tilde{h}_j, j = 0, \dots, N$, while the channel impulse response could have some precursors which are different from 0, and therefore an anticausal component. If we assume that $N_1 = \hat{N}_1$, $N_2 = \hat{N}_2$ and $N = \hat{N} = \hat{N}_1 + \hat{N}_2 + 1$, in order to correctly estimate $h_i, \; i = -N_1, \dots, N_2$ LS considers $a_k, \; k = 0, \dots, L+\hat{N}-1$ and the output $x_k, \; k = D_{ch} - \hat{N}_1, \dots, D_{ch} - \hat{N}_1 + L + \hat{N} - 1$ and it performs the estimation in the standard way. This is the same as considering a causal impulse response, with $\hat{N}$ coefficients, but since we suppose that $N_1$ assumes a given value $\hat{N}_1$ in $[0, D_{ch}]$, then one can shift left the estimate by $N_1$ samples and obtain the estimate for $\hat{h}_i, \; i = -N_1, \dots, N_2$. 

The actual estimation is performed as in~\cite[p.~246]{bc}, i.e. using a $L \times \hat{N}$ observation matrix
\begin{equation}
	\boldsymbol{\mathcal{I}} =
 \begin{bmatrix}
  a(\hat{N}-1) & \cdots & a(0) \\
  \vdots  & \ddots & \vdots  \\
a[(\hat{N}-1)+(L-1)] & \cdots & a(L-1) \\
 \end{bmatrix}
\end{equation}
and a desired sample vector
\begin{equation}
\mathbf{o}^T = \left[ x(\hat{N}-1)\;,\; \ldots\; , \;x((\hat{N}-1)+(L-1)) \right].
\end{equation}
Therefore we can solve the LS problem as
\begin{equation}
	\hat{\mathbf{h}}= \mathbf{\Phi}^{-1} \boldsymbol{\vartheta}, \quad\mathrm{ with } \quad \mathbf{\Phi}=\boldsymbol{\mathcal{I}}^H \boldsymbol{\mathcal{I}} \quad \mathrm{ and }\quad \boldsymbol{\vartheta} = \boldsymbol{\mathcal{I}}^H \mathbf{o}.
\end{equation}
In particular, in order to estimate the length $\hat{N} = N_1 + N_2 + 1$ of the impulse response and $N_1, N_2$, we operate as follows. Let $N_{max} = N_{seq} = 10$ the maximum possible length $\hat{N}$ of the impulse response. At first, we set $N_1 = 0$ and compute the functional of the error
\begin{equation} 
	\mathcal{E} = \frac{1}{L}\sum_{k = \hat{N} - 1}^{(L-1)+(\hat{N}-1)}|x_k-\hat{x}_k|^2
	\label{eq:functional}
\end{equation}
for $N_2 = 0, 1, \dots, N_{max} - N_1 - 1$. This procedure is repeated for $N_1 = 1$ and $2$. The functionals $\mathcal{E}$ for different $N_1$ are then plotted together, against $N_2$, in Fig.~\ref{fig:functional}.
\begin{figure}[h!]
	\centering
	\includegraphics[width = 0.7\textwidth]{error_func_p1}
	\caption{$\mathcal{E}$ vs $N_2$ for different $N_1$}
	\label{fig:functional}
\end{figure}
For any $N_1$ there is a knee at $N_2 = 4$, therefore we pick $N_2 = 4$. Then the $N_1$ that minimizes $\mathcal{E}$ should be chosen, therefore $N_1 = 2$. However the 3 curves are very close to each other, and choosing $N_1 = 0$ would bring a similar functional $\mathcal{E}$ but a much lower complexity for Viterbi and FBA in the second problem. 

In Tables~\ref{table:h0} and~\ref{table:h2} are reported the amplitude and phase of $h_i, i = -N_1, \dots, N_2$, while in Fig.~\ref{fig:hi_p1} is reported the amplitude of both the choice $N_1 = 0$ and $N_1 = 2$. Note that they are very similar.


% !!! Riportare valori di {h_i_hat} in una tabella, ampiezza e fase
\begin{table}[h!]
	\centering
	\begin{tabular}{c|c|c|c|c|c}
		$i \in [-N_1, N_2]$ & 0 & 1 & 2 & 3 & 4 \\ \hline
		$|h_i|$ 		& 0.7140  &  0.2463  &  0.1553  &  0.5778  &  0.4001 \\
		$\angle(h_i)$ 	& -2.5815  &  1.3137  & -2.5666  & -1.5096  & -1.6715 \\
	\end{tabular}
	\caption{Amplitude and phase of $h_i, i \in [-N_1, N_2]$, for the choice $N_1 = 0$}
	\label{table:h0}
\end{table}

\begin{table}[h!]
	\centering
	\begin{tabular}{c|c|c|c|c|c|c|c}
		$i \in [-N_1, N_2]$ & -2 & -1 & 0 & 1 & 2 & 3 & 4 \\ \hline
		$|h_i|$ 	&	0.0121  &  0.0137  &  0.7136   & 0.2446  &  0.1551  &  0.5853  &  0.4068 \\
		$\angle(h_i)$ &	0.3575  & -2.0494  & -2.5692   & 1.3064  & -2.5498  & -1.5017  & -1.6574 \\
	\end{tabular}
	\caption{Amplitude and phase of $h_i, i \in [-N_1, N_2]$, for the choice $N_1 = 2$}
	\label{table:h2}
\end{table}

% !!! Plottare {h_i_hat} per i = -N_1, ..., N_2
\begin{figure}[h!]
 \centering
 \includegraphics[width = 0.7\textwidth]{hi_p1}
 \caption{$|h_i|, i \in [-N_1, N_2]$ for $N_1 = 0$ and $N_1 = 2$}
 \label{fig:hi_p1}
\end{figure}

% !!! Determinare la stima della varianza del rumore, sigma_w_hat_squared, compararla con sigma_w_squared in dB.
The noise that the channel introduces is $\sigma_w^2 = -19.2411$ dB, while the noise estimate is lower: $\hat{\sigma}_w^2 = -20.5327$ dB for the choice $N_1 = 0$, and $\hat{\sigma}_w^2 = -20.851$ for the choice $N_1 = 2$ (see Eq.~\ref{eq:functional}). Note that this is the value given by a single realization. However, this shows that a sequence of the given length $L = 15$ is too short to correctly estimate the real noise of the channel, since it averages on a number of noise samples which is close to the length of the impulse response we are trying to estimate. Indeed, by using a longer ML sequence (e.g. 127) the resulting estimate is much closer to the actual value, even considering one single realization. 

% !!! Comparare la stima di Lambda_n_hat con il valore "vero" di Lambda_n_hat, fare le considerazioni su LS per cui il Lambda_n teorico raddoppia quando si usano i simboli ±1±j al posto di ±1.
At last, it is necessary to make some considerations on the fact that the sequence used in this problem is a binary mapping of a PN sequence to a QPSK constellation with $\sigma_a^2 = 2$. In particular let $a_l = (1+j) p_l$ with $p_l = \pm 1$ a symbol of the PN sequence. Then the autocorrelation of the sequence, of periodicity L, is
\begin{equation}
	r_a(n) = \frac{1}{L}\sum_{l = 0}^{L-1} a_l a^*_{l-n} = \\
	\frac{1}{L}\sum_{l = 0}^{L-1} (1+j)p_l (1-j)p^*_{l-n} = 
	\frac{2}{L} \sum_{l= 0}^{L-1} p_l p^*_{l-n} = 2 r_p(n)
\end{equation}
with 
\begin{equation}
	r_p(n) = 
  	\begin{cases}
    1       & \quad \text{if } n =0 \\
    -\frac{1}{L}  & \quad \text{if } n \neq 0. \\
  \end{cases}
\end{equation}
Therefore let $\Phi(i, n) = [\mathbf{\Phi}]_{i, n}$. Then
\begin{equation}
	\Phi(i, n) = (L -1 + N - N + 1)r_a(i-n) = L \, 2 \, r_p(i-n).
\end{equation}
At the same time $\vartheta(n) = L \, 2 \, r_{xp}(n)$ with $\boldsymbol{\vartheta} = [\vartheta(0), \dots, \vartheta(N-1)]$.
Thus the LS can be solved in the usual way, as previously mentioned, with the equation $\hat{\mathbf{h}}= \mathbf{\Phi}^{-1} \boldsymbol{\vartheta}$ and provides the same solution that a PN sequence would give. 

As for the signal-to-estimation error ratio, it is possible to proceed as in \cite{bc}. Let $\boldsymbol{\vartheta} = \sum_{N-1}^{N-1+L-1} x(k) \mathbf{a}^*(k)$ and $\boldsymbol{\xi} = \sum_{N-1}^{N-1+L-1} w(k) \mathbf{x}^*(k)$. We obtain $\boldsymbol{\vartheta} = \boldsymbol{\Phi}\mathbf{h} + \boldsymbol{\xi}$ and finally $\boldsymbol{\Delta h} = \boldsymbol{\Phi}^{-1} \boldsymbol{\xi}$. 
Let's now consider the energy of this vector. Since $w$ is zero mean, with variance $\sigma_w^2$, then
\begin{equation}
	\mathbf{R}_{\boldsymbol{\xi}} = E[\boldsymbol{\xi}^*\boldsymbol{\xi}^T] = \sigma_w^2 \mathbf{\Phi}^*
\end{equation}
and
\begin{equation}
	\mathbf{R}_{\boldsymbol{\Delta h}} = \sigma_w^2 (\mathbf{\Phi}^*)^{-1}.
\end{equation}
Then we need $E[||\mathbf{\Delta h}||^2]$ which is
\begin{equation}
	E[||\mathbf{\Delta h}||^2] = \sigma_w^2 \tr[(\mathbf{\Phi}^*)^{-1}].
\end{equation}
Since the definition of signal-to-estimation error ratio is 
\begin{equation}
	\Lambda_n = \frac{\sigma_w^2}{M_a E[||\mathbf{\Delta h}||^2]}
\end{equation}
with $M_a = 2$ for a QPSK, then in this specific case
\begin{equation}
	\Lambda_n = \frac{\tr[(\mathbf{\Phi})^{-1}]^{-1}}{M_a}.
\end{equation}

Since $\mathbf{\Phi}$ has the same form of the $\mathbf{\Phi}$ matrix for a PN sequence, except for a multiplicative factor $2$, we can use the same derivation of \cite{bc} in order to compute $\Lambda_n$. Let $\mathbf{1}_{N\times N}$ a matrix with all ones, then
\begin{equation}
	\mathbf{\Phi} = 2 ((L+1)\mathbf{I} - \mathbf{1}_{N\times N})
\end{equation}
and
\begin{equation}
	\mathbf{\Phi}^{-1} = \frac{2}{L+1} \left( \mathbf{I} + \frac{\mathbf{1}_{N\times N}}{L+1-N} \right).
\end{equation}
Finally, following \cite{bc}, we obtain
\begin{equation}
	\Lambda_n = \frac{2}{M_a}\frac{(L+1)(L+1-N)}{N(L+2-N)} = \frac{(L+1)(L+1-N)}{N(L+2-N)}
	\label{eq:lambdateo}
\end{equation}
getting the same result as for a PN sequence.

In conclusion, some numerical results are reported in Table~\ref{table:lambda}, both for the choice $N_1 = 2$ given by the minimum of the functional and for $N_1 = 0$ given by the practical considerations mentioned above. The first column of Table~\ref{table:lambda} contains the estimated value computed as
\begin{equation}
	\Lambda_n = \frac{\hat{\sigma}_w^2}{M_a E[||\mathbf{\Delta h}||^2]}.
	\label{eq:lambdareal}
\end{equation}
Note that $E[||\mathbf{\Delta h}||^2]$ is computed by assuming the true $h_i$ is known and that the value of \eqref{eq:lambdareal} is affected by the bad estimation of $\sigma_w^2$ given by the choice of a value of $L$ which does not provide a long enough training sequence.

\begin{table}[h!]
	\centering
	\begin{tabular}{c|c|c}
				&	As in Eq.~\eqref{eq:lambdareal} [dB]	& As in Eq.~\eqref{eq:lambdateo} [dB]  \\ \hline
	$N_1 = 2$	&	1.82							& 3.13 \\
	$N_1 = 0$	&	1.87							& 1.66 \\
	\end{tabular}
	\caption{$\Lambda_n$ for $N_1 = 0$ and $N_1 = 2$}
	\label{table:lambda}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% PROBLEM 2 %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Problem 2}

% Breve descrizione del sistema in T
In this second problem the system works in T, as in Figure~\ref{fig:channelt}, and as SNR we consider $\Gamma = \frac{\sigma_a^2 E_h}{\sigma_w^2}$, where $E_h = 1.0665$.
\begin{figure}[hb]
	\centering
	\includegraphics[width=0.5\textwidth]{channel_t}
	\caption{System description for the second problem}
	\label{fig:channelt}
\end{figure}
The output is computed using a sampled version of the provided impulse response $q(nT_c)$ that matches the response estimated in the previous problem, i.e. $h_i = q(iT + m_{opt}), \quad i \in [-N1, N2]$. The sequence $a_k$ which is transmitted is composed of a ML binary sequence of 25 symbols, as in Problem 1, which is necessary to perform the channel estimate, and of a QPSK stream of symbols that maps a ML sequence composed of 0s and 1s in order to mimic random transmission of bits. In case more than $2^{20}-1$ bits are needed we partially or completely repeat the longest ML sequence of $2^{20}-1$ bits.

Note that in the following analysis we will consider always the same parameters for the channel, with $N_1 = 0, N_2 = 4$ accordingly to the considerations of Problem 1. However the estimate must be repeated for each different SNR, since the channel noise varies. Moreover, in order to design the system, we consider only the estimated impulse response and not the real one, and since the estimate done with a ML sequence of $L = 15$ is lousy, then also the design will not drive the performance of the receivers as close as possible to the theoretical bound. 

This is computed as in \cite{bc} for the matched filter case (i.e. no ISI) and a QPSK constellation ($M=4$), resulting in 
\begin{equation}
	BER_{MF} = \frac{1}{\log_2 (M)} 4 \bigg(1- \frac{1}{M} \bigg) Q\bigg(\sqrt{\frac{3}{M-1} \Gamma}\bigg) = Q(\sqrt{\Gamma})
	\label{eq:BERmf}
\end{equation}
where $\Gamma$ is the SNR at receiver input, which in the case of a matched filter is equal to $\Gamma = 0.5 M_a \gamma_{MF}= \gamma_{MF}$, with $\gamma_{MF}$ the SNR at detection point for a MF, $M_a = \sigma_a^2 = 2$ since the transmitted sequence has 0 mean.

We also computed this bound with a simulation, where the receiver performed a decision over received data with no ISI, $x_k = a_k + w_k$, with $a_k$ a sent symbol, $w_k$ AWGN noise of power $\sigma_w^2 = \sigma_a^2 \ \Gamma$ in order to match the SNR $\Gamma$ of the real channel (which has the $E_h$ factor since the data stream is convolved with the $h_i$ filter). Decision is taken with a threshold detector.


% Considerazioni sull'implementazione di LE e DFE, riportare i grafici di Jmin (qualche grafico), considerazioni su M1, M2, D, riportare i grafici di \hat{h}, c, psi, b per \Gamma = 10 dB
\subsection*{Linear Equalizer and Decision Feedback Equalizer}
The simplest receivers implemented are a linear equalizer (LE) and a decision feedback equalizer (DFE) which try to remove the ISI component from the received samples $x_k$. The decision is then taken with a threshold detector.  They both use the information known about the channel in order to perform equalization. In particular the LE tries to equalize both the precursors and the postcursors out of the received signal $x_k$, while DFE uses the detected symbols to perform cancellation of the postcursors from a signal already filtered with a feed-forward filter that removes the precursors. Figures~\ref{fig:LE} and~\ref{fig:DFE} contain the block scheme for both of them. 
\begin{figure}[h!]
	\centering
	\includegraphics[width = 0.7\textwidth]{LE}
	\caption{LE filter}
	\label{fig:LE}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\textwidth]{DFE}
	\caption{DFE filter}
	\label{fig:DFE}
\end{figure}

Let the filter $c$ have length $M_1$ and an added delay equal to $D$, and the feedback filter $b$ have length $M_2$. Then the LE can be seen as a DFE with $M_2 = 0$. The filter coefficients are indeed computed in the same way, using the Wiener-Hopf theory and the MSE criterion applied to $J = E[|a_{k-D} - y_k|^2]$. Note that we compare $a_{k-D}$ with $y_k$ because of the delay D introduced by the c filter, therefore at the time $k$ at detection point the filter decides on $a_{k-D}$. The final equations that are implemented in the \texttt{DFE\_filter} function are the ones that solve the Wiener-Hopf equation, in particular
\begin{equation}
	[\mathbf{p}]_p = \sigma_a^2 \hat{h}^*_{D-p}, \quad \quad p = 0, 1, \dots, M_1 - 1
\end{equation}
and
\begin{equation}
	[\mathbf{R}]_{p, q} = \sigma_a^2 \bigg(\sum_{j = -N_1}^{N_2}\hat{h}_j \hat{h}^*_{j-(p-q)} - \sum_{j = 1}^{M_2} \hat{h}_{j+D-q}\hat{h}^*_{j+D-p} \bigg) + r_{w}(p-q), \quad \quad p,q = 0, 1, \dots, M_1 - 1
\end{equation}
with $r_w(p-q)$ the autocorrelation of the noise imposed by the receiver front-end filter. Since in this case the receiver filter is not present, i.e. $g_{RC}(t) = \delta(t)$, and it does not change the shape of the noise. Therefore $r_w(p-q) = \hat{\sigma}_w^2 \delta_{p-q}$ since $w_k$ is white Gaussian of estimated power $\hat{\sigma}_w^2$. Then the optimum feed-forward filter is given by
\begin{equation}
	\mathbf{c}_{opt} = \mathbf{R}^{-1}\mathbf{p}
\end{equation}
and for the DFE the feedback filter is given by 
\begin{equation}
	b_i = -\sum_{l=0}^{M_1-1} c_{opt, l} \hat{h}_{i+D-l}, \quad \quad i = 1, 2, \dots, M_2
\end{equation}

Finally, it is possible to compute in a close form the functional of the error, 
\begin{equation}
	J_{min} = \sigma_a^2 - \sum_{l=0}^{M_1-1} c_{opt, l}[\mathbf{p}]^*_l = \sigma_a^2 \left( 1 - \sum_{l=0}^{M_1-1}c_{opt, l}\hat{h}_{D-l} \right).
\end{equation}
This is what drove us in the design of the filter and in the choice of the coefficients $M_1, M_2, D$. In particular we considered the parameters which minimize the functional for the lowest SNR $\Gamma$ considered, and check that they do the same also for the functional at different $\Gamma$. If not, we update them accordingly in order to have filters that work in the best way for all the $\Gamma \in [5, 15]$ dB considered. In Figures~\ref{fig:jminle} and~\ref{fig:jmindfe} two of these functionals for LE and DFE are shown. Note that the choice of the pair $M_1, D$ is given by the pair of minimum values for which the functional presents a minimum or a knee. For example, in Figure~\ref{fig:jminle}[b], the couple $M_1 = 20, D = 15$ guarantees a low $J_{min}$, and by increasing the value of the two parameters there is not a considerable gain in the performances. At the same time, in Figure~\ref{fig:jmindfe}, we observe that by choosing a $D > M_1 - 1$ we do not get better results, therefore we set $D = M_1 - 1$ for the DFE. Note that $M_2 = 0$ for LE, while for the DFE is given by the other parameters, i.e. $M_2 = N_2 + M_1 - 1 - D$ in order to be sure to cancel out all the postcursors (at least the ones accounted for by the estimate of $N_2$). The choice of the parameters is reported in Table~\ref{table:ledfe}. 

% We also run some simulations in which we changed the parameters 

\begin{figure}
	\centering
	\subfigure[$\Gamma = 5$]{\includegraphics[width= 0.42\textwidth]{jmin_LE_5}}
	\subfigure[$\Gamma = 15$]{\includegraphics[width= 0.42\textwidth]{jmin_LE_15}}
	\caption{$J_{min}$ for LE}
	\label{fig:jminle}
\end{figure}

\begin{figure}
	\centering
	\subfigure[$\Gamma = 5$]{\includegraphics[width= 0.42\textwidth]{jmin_DFE_5}}
	\subfigure[$\Gamma = 15$]{\includegraphics[width= 0.42\textwidth]{jmin_DFE_15}}
	\caption{$J_{min}$ for DFE}
	\label{fig:jmindfe}
\end{figure}


\begin{table}[h!]
	\centering
	\begin{tabular}{c|c|c|c}
	Filter 	& 	$M_1$	& 	$D$		&		$M_2$		\\ \hline
	LE 		&	20		&	15		&		0			\\
	DFE 	&	25		&	24		&		4			\\
	\end{tabular}
	\caption{Filter parameters}
	\label{table:ledfe}
\end{table}

In order to visualize the equalization brought by these filters we plotted the estimated impulse response $h_i$, both for LE and DFE, in Figure~\ref{fig:hledfe}. The feed-forward filter $c$ and the convolution between $h_i$ and $c$, which is $\psi_i = (h*c)_i$ for LE are in Fig.~\ref{fig:LEcoeff}, whereas in Fig.~\ref{fig:DFEcoeff} $c$, $\psi$ and the coefficients $b$ of the feedback filter of the DFE are shown. Note that the LE should try to equalize both postcursors and precursors, but in this case only the former are present. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\textwidth]{h_LEDFE}
	\caption{Estimated $|h|$ for LE and DFE coefficients computation}
	\label{fig:hledfe}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width = 0.7\textwidth]{cphiLE}
	\caption{$|c|$ and $|\psi|$ for LE}
	\label{fig:LEcoeff}
\end{figure}

\begin{figure}[h!]
	\centering
	\subfigure{\includegraphics[width = 0.7\textwidth]{cphiDFE}}
	\subfigure{\includegraphics[width = 0.7\textwidth]{bDFE}}
	\caption{$|c|$, $|\psi|$, $|b|$ for DFE}
	\label{fig:DFEcoeff}
\end{figure}

\clearpage

% Considerazioni su Viterbi, Kd, grafico della distribuzione
\subsection*{Viterbi algorithm}

The Viterbi algorithm was implemented as a MATLAB function that performs the maximum likelihood detection using the whole message, including the known PN preamble for channel estimation, with initial conditions set to zero. Therefore, the initial conditions for the detection of the actual data (i.e. without the preamble) are given by the final symbols of the training sequence, which also has the function of training the Viterbi algorithm. The parameters $L_1 = 0$ and $L_2 = 4$ are set equal to $N_1$ and $N_2$ from problem 1.

The state of the FSM associated with the algorithm at time $k$ is as in \cite{bc}:
\begin{equation}
	\mathbf{s}_k = (a_{k+L1}, \ldots, a_{k-L_2+1}) = (a_k, \ldots, a_{k-3}), \quad k=0...K-1,
	\label{eq:state}
\end{equation}
where $K$ is the length of the input sequence, therefore the number of states is $N_S = M ^ {L_1 + L_2} = 4^4 = 256$. We associate each symbol of the QPSK constellation to an index $\alpha = 0,\ldots,3$ (the rule is arbitrary and it does not affect the result), so that a symbol $a_k$ at time $k$ is mapped to the corresponding index $\alpha_k$. The internal representation of the state is an integer in base $M$, with the most recent symbol in the least significant position, such that the state $\mathbf{s}_k$ is mapped to the integer
\begin{equation}
s_k = \sum_{i=0}^{L_1+L_2-1} M^i \alpha_{k+L_1-i} = \sum_{i=0}^{3} M^i \alpha_{k-i}.
\end{equation}
At time $k$, given the state $\mathbf{s}_{k-1}$ represented by $s_{k-1}$, and assuming the new symbol is $a_k$, the new state index is obtained by setting to 0 the two most significant bits (corresponding to the oldest symbol), shifting left by 2 bits and then adding the index of the new symbol in the two least significant bits:
\begin{equation}
s_k = \left[ (s_{k-1}) \lmod{M^{L_1+L_2-1}} \right] M + \alpha_k = 4 \, \left[ (s_{k-1})\lmod{64} \right] + \alpha_k.
\label{eq:newstateexpression}\end{equation}

In our implementation, we check all possible transitions by cycling through all state indices $s_{k-1}$ and considering for each of them the $M=4$ new possible symbols. Let the sequence of new state indices $s_k$ defined with this double iteration be $\{ s_k^{(i)} \}_{i=0,\ldots,MN_S-1}$, that has length $MN_S = M^{L_1+L_2+1} = 1024$.  Then, we have
\begin{equation}
s_k^{(i)} = i \mod N_S,
\end{equation}
hence a simple way of defining the new state indices is initializing a counter to 0 and incrementing it at every iteration of the inner loop, setting it again to 0 every time it reaches the value $N_S$. In fact, let us examine the first $N_S$ iterations, that correspond to $\frac{N_S}{M}$ iterations of the outer loop, in which the state indices $s_{k-1} = 0,\ldots,\frac{N_S}{M}-1$ are considered. Then, the first term of \eqref{eq:newstateexpression} takes the sequence of values
\begin{equation}
0,\,M,\,2M,\,\ldots,\,\left(\frac{N_S}{M}-1\right)M,
\end{equation}
that only has multiples of $M$. For each of them, the inner loop adds a term that takes values $0,\ldots,M-1$. Hence, the overall sequence cycles through all integers from $0$ to $N_S-1$, and the same reasoning applies for iterations $N_S +1$ to $MN_S$ -- the only difference is in the 2 most significant bits of $s_{k-1}$ that are discarded by the modulo operation. Since in our case $M=4$, this wrapping of the sequence happens 4 times.

Using the notation of \cite{bc}, we define $u_k$ as the desired received signal that carries information, that is the convolution of the sent symbol with the impulse response of the channel:
\begin{equation}
u_k = \sum_{n=-L_1}^{L_2} h_n a_{k-n} = \sum_{n=0}^{4} h_n a_{k-n}.
\label{eq:ukdefinition}\end{equation}
We observe that, given a fixed impulse response of the channel, $u_k$ only depends on the symbols $a_{k-4},\ldots, a_k$, which equivalent to say that it is a function of the current and the previous state, or of the current symbol and the previous state, i.e.
\begin{equation}
u_k = f_1(\mathbf{s}_{k-1}, \mathbf{s}_k) = f_2(\mathbf{s}_{k-1}, a_k) = f(s_{k-1}, \alpha_k),
\end{equation}
where $f$ has support $\{0,\ldots,N_S-1\} \times \{0,\ldots,M-1\}$. In the initialization phase of the algorithm, we build a $N_S \times M$ matrix, according to \eqref{eq:ukdefinition}, to be used during execution as a lookup table to determine the value of $u_k$ given the previous state and the current assumed received symbol. Note that in the first iterations $u_k$ depends on the initial conditions on the sequence $\{a_i\}$, that in our case were set to zero, therefore the lookup table is formally inaccurate. However, this transient error disappears completely before the end of the training sequence, thus the detection of the actual data is formally and practically correct.

At time $k$, for each possible previous state $\mathbf{s}_{k-1}$, the cost metric
\begin{equation}
\Gamma_k = \sum_{i=0}^k |\rho_i - u_i|^2
\end{equation}
is computed recursively as
\begin{equation}
\Gamma_k = \Gamma_{k-1} + |\rho_k - u_k|^2,
\end{equation}
where $\rho_k$ is the observed sample at time $k$ and $\Gamma_{k-1}$ is the cost metric of the assumed state $\mathbf{s}_{k-1}$. The 4 possible values of $u_k$ are given by row $s_{k-1}$ of the lookup table, yielding 4 different values of $\Gamma_k$ according to the new symbol $a_k$. These 4 values of $\Gamma_k$ are always associated to different states $\mathbf{s}_k$, and each of the new states $\mathbf{s}_k$ has 4 values of $\Gamma_k$ depending on the transition the system used to get to this state. Instead of storing all $MN_S$ values of $\Gamma_k$, we only keep track of the lowest $\Gamma_k$ for each new state $\mathbf{s}_k$, and we record the predecessor $\mathbf{s}_{k-1}$ (by means of its index $s_{k-1}$) that yielded the lowest cost. Finally, we make the decision on the oldest symbol of the survivor sequences according to the cost metric $\Gamma_k$, and we store the new survivor sequences with the respective costs.

The survivor sequences at time $k$ are stored as symbols of the constellation, i.e. $a_{k-K_d} \ldots,a_{k-1}$, and the symbol $a_{k-K_d}$ is decided at the end of the $k$-th iteration. Therefore the Viterbi algorithm has a finite memory, that is the length $K_d$ of the trellis diagram, also known as trellis depth. It may happen that at time $k$ the symbols $a_{k-K_d}$ of all survivor sequences are the same, which means that the $(k-K_d)$-th symbol, and the previous ones, are already fixed: there is no point in keeping them in the survivor sequences, so we discard them and store the correct symbol in the vector of decided data. On the other hand, if the symbols $a_{k-K_d}$ of the survivor sequences are different then we are deciding based on the path metric up until $k$ (that is $\Gamma_k$), whereas we should decide the whole sequence according to the path metric computed on the whole message. This is an approximation that can in general yield some inaccuracies in the detection. The goal is to minimize $K_d$ for efficiency, without affecting too much the results.

We run 16 simulations of the transmission of 3 ML sequences with length $2^{20} - 1$ over our channel, with the setup used for all experiments of problem 2, and a Viterbi detector with $K_d = 80$. At each iteration (starting after the transient $K_d$), the modified Viterbi detector checks which is the minimum value of $K_d$ that would yield no premature decision. All survivor sequences contain the same symbols from a certain column backwards, and that column is given by the minimum $K_d$ needed.

Finally, we average the results of all experiments to obtain a complementary CDF for the minimum $K_d$. If the resulting complementary CDF is $y$ for a certain value of $K_d$, then for that $K_d$ the number of iterations in which the decision on the oldest symbol is actually made among different symbols is $y$ times the total number of iterations (minus the number of iterations of the transient, during which the survivor sequence is filled for the first time). In those cases, the symbol that belongs to the most likely sequence is chosen, which might turn out to be a suboptimal decision. The number of times we make this approximation on the Viterbi algorithm is $y$ times the length of the sequence.

The complementary CDF we get is shown in Fig.~\ref{fig:Kd_min_estimation}, from which we chose $K_d = 28$, that yields a probability of at most $7\cdot 10^{-7}$ that the first column of the matrix has different symbols. Therefore, the probability of making a wrong choice due to this approximation cannot be more than $7\cdot 10^{-7}$, which is reasonable unless we are trying to measure BER's on that order of magnitude.

\begin{figure}
 \centering
 \includegraphics[width = 0.6\textwidth]{Kd_min_estimation}
 \caption{Complementary empirical CDF of $K_{d,\mathrm{min}}$ for the choice of $K_d$, over 16 independent simulations.}
 \label{fig:Kd_min_estimation}
\end{figure}


%%% Considerazioni su FBA (dettagli implementativi)
% - Considerazioni sull'occupazione di memoria e sulla velocità in riferimento al Viterbi
\subsection*{Max-Log-MAP}
Using the maximum a posteriori probability (MAP) criterion, the decision upon a received symbol is performed according to the expression:
\begin{equation}
	\hat{\boldsymbol{a}} = \argmax_{\boldsymbol{\alpha}} P[\boldsymbol{a} = \boldsymbol{\alpha} | \boldsymbol{z} = \boldsymbol{\rho}]
\end{equation}
where $\hat{\boldsymbol{a}}$ is the sequence of the detected symbols, $\boldsymbol{a}$ is that of the sent symbols, $\boldsymbol{\alpha}$ are the values of the sent symbols, $z$ is the sequence of the received symbols and $\boldsymbol{\rho}$ are the values of the received symbols.
If we introduce the \emph{likelihood function}
\begin{equation}
	\mathtt{L}_k(\beta) = P[a_{k+L_1} = \beta | \mathbf{z}_0^{K-1} = \boldsymbol{\rho}_0^{K-1}]
\end{equation}
the criterion becomes:
\begin{equation}
	\hat{a}_{k+L_1} = \argmax_{\beta \in \mathcal{A}} \mathtt{L}_k(\beta)
\end{equation}
In order to solve the problem of the likelihood function maximization, the FBA algorithm can be used. The algorithm performs the computation of four metrics, namely:
\begin{itemize}
	\item The Forward metric, $F_k(j)$
	\item The Backward metric, $B_k(j)$
	\item The State metric, $V_k(j)$
	\item The Likelihood function of the generic symbol,
	\begin{equation}
	 	\mathtt{L}_k(j) = \sum_{\substack{i = 1 \\ [\boldsymbol{\sigma}_i]_1 = \beta}}^{N_S} V_k(i) \quad \beta \in \mathcal{A}
	 \end{equation} 
\end{itemize}
where k is the variable associated with the symbol index and j is the state index, as defined in the Viterbi algorithm~\eqref{eq:state}.

Applying the logarithm to the above metrics yields the \emph{log-likelihood} function, whose expression is 
\begin{equation}
	\ell_k(\beta) = \ln \mathtt{L}_k(\beta) = \ln \sum_{\substack{i = 1 \\ [\boldsymbol{\sigma}_i]_1 = \beta}}^{N_S} e^{v_k(i)} \quad \beta \in \mathcal{A}
\end{equation}
where $v_k(i) = \ln V_k(i)$ is the logarithm of the state metric.

Using the logarithm the difference between the state metrics is enhanced, and as a result one term usually dominates within each time index $k$ in the sum above. Therefore, we can introduce the Max-Log-MAP criterion:
\begin{equation}
	\hat{a}_{k+L_1} = \argmax_{\beta \in \mathcal{A}} \tilde{\ell}_k(\beta)
	\label{eq:decision}
\end{equation}
where the following approximation holds:
\begin{equation}
	\tilde{\ell}_k(\beta) = \max_{\substack{i \in [1, \dots, N_S] \\ [\boldsymbol{\sigma}_i]_1 = \beta}} v_k(i)
	\label{eq:likelihood}
\end{equation}

In our implementation we perform three main cycles in order to get all the metrics needed to compute $\tilde{\ell}$. 

The first cycle computes the channel transition metrics, storing them in a $M \times N_S \times K$ matrix where for a fixed time $k = 1, \dots, K$ the $M$ elements in the j-th column represent the transitions metrics from the state j. The metrics are computed using the following expression: 
\begin{equation}
	c_k(j|i) = - |\rho_k - u_k|^2
\end{equation}
Since the desired signal $u_k$ is just a function of the current and next state, its values are precomputed like in the Viterbi implementation~\eqref{eq:ukdefinition} and stored in a lookup table. $\rho_k$ represents the k-th sample of the received signal. For each time $k$, the $j$ index refers to the destination state, while the $i$ index refers to the starting state. Using matrix operations in MATLAB, we can fill the $c$ matrix by only iterating over time. For $k = K$ all values are set to 0.

The second cycle computes the Backward metric. A matrix of size $N_S \times K+1$ is created and initialized to zero. This also initializes the final, K-th state to the 0 value, since the final symbol that will be received is unknown (performance of the algorithm may be enhanced by transmitting a previously decided symbol, also known to the receiver). The algorithm cycles the symbol indices from $K-1$ to $0$, iterating over the states and updating their backward metric according to the following expression:
\begin{equation}
	\tilde{b}_k(i) = \max_{m \in [1, \dots, N_S]}[\tilde{b}_{k+1}(m) + c_{k+1}(m|i)] \quad i = 1, \dots, N_S
\end{equation}
At each time $k$, the indices $m$ of the states which are reachable from the current one are needed. These indices are computed using the expression $m = ((i-1) \lmod{M^{L_1+L_2-1}}) * M + j$, where $i$ is the current state and $j = 0, \dots, M-1$ is the index of the transition. On the other hand, the $c_{k+1}(m|i)$ coefficients are readily available as the i-th column of the channel transition metrics at time $k+1$.

The final cycle is used to compute the $\tilde{f}_k(j)$ metric for each state at each symbol index $k$. Since the State metric is $\tilde{v}_k(i) = \tilde{f}_k + \tilde{b}_k(i)$ for $i = 1, \dots, N_S$, there is no need to memorize $\tilde{f}_k(i)$ for every time index. A single iteration is performed in order to cover all the symbol indices from $0$ to $K-1$. For each $k$, another iteration over the $N_S$ states fills the vector of the forward metrics for all the states of the current $k$ index using the equation
\begin{equation}
	\tilde{f}_k(j) = \max_{\ell \in [1, \dots, N_S]} [\tilde{f}_{k-1}(\ell) + c_k(j|\ell)] \quad j = 1, \dots, N_S
\end{equation}
and the state metric is computed by adding this vector to the one memorized during the previous cycle, containing the backward metric. To get the channel transition metrics $c_k(j|\ell)$, the indices $\ell$ of the states that can reach the current one are necessary. These indices are computed according to the following expression: $\ell = \ceil{\frac{j}{M}} + r*M^{L_1+L_2-1}$, where $r = 0, \dots, M-1$. 
After computing the State metric $\tilde{v}_k(i) = \tilde{f}_k + \tilde{b}_k$ for $i = 1, \dots, N_S$, the likelihood function is obtained using~\eqref{eq:likelihood}: in order to decide which state metrics should be compared, we make use of a precomputed matrix of size $N_S \times M$ which contains the symbols associated to each state and pick only those states whose initial symbol is $\beta$. Finally, the decision is performed inside the same cycle according to~\eqref{eq:decision}, and the result is memorized in a vector. Alternatively, the values of the likelihood function may be used as input for  a decoder to perform \emph{soft decoding}.

Memory occupation for the Max-Log-MAP criterion is a concern. Since the Backward and channel transition matrices are memorized, their total memory occupation would be of $N_S \times N_S \times K + N_S \times K$ memory cells, however our implementation only represents the transitions that are actually possible, thus reducing the memory occupation to $M \times N_S \times K$ cells for the channel transition metric. It could have been possible to perform the computation of the channel transition metrics in a sliding window fashion, only computing the needed metrics once in a while instead of computing them all together, in the beginning of the algorithm. However, memory occupation issues were mainly solved by only memorizing the actual transitions, so computational speed was preferred. 
Computation times for the Max-Log-MAP have been more or less twice those of the Viterbi algorithm for sequences of the same length.
% TODO Refine this stuff above here

% Grafici del BER, sia per canale stimato che per canale vero, riportando il numero di bit per le simulazioni

\subsection*{Bit Error Rate plots}
The BER for different scenarios is finally simulated. In Figure~\ref{fig:BER_same} a comparison is shown among the BER's of the four different receivers given the same estimate for the channel impulse response, for each value of the SNR $\Gamma$. The bit sequences used are ML sequences mapped into a QPSK constellation, whose length is in Table~\ref{table:BER_same_len}. They are chosen long enough to get at least 30 errors for each simulation, but not so long as to produce unnecessary computations. Actually, since the ML sequences have an odd length and a QPSK symbol is composed of two bits, then if the ML sequence has length $2^r - 1$ the transmitted bits will be $2^r - 2$.

Note that for a single estimate of the channel the results are not too close to the theoretical bound. Indeed, as previously mentioned, the estimate performed with a short training sequence is lousy and this affects the performances of the receivers. The same behavior can be seen in Figure~\ref{fig:BER_avg} where we average the BER for different realizations and estimates of the channel, by calling multiple times with different RNG seeds the \texttt{BER\_same\_channel} script. This can be done only because the impulse response doesn't change over time, otherwise it would not be meaningful. The idea behind this kind of simulation is to study the effect of the bad channel estimate on BER without considering just one realization.

Note that LE and DFE tend to perform in the same way up to 10 dB, then the cancellation of the DFE becomes more accurate and its BER becomes lower than the one of LE. At the same time, the Viterbi algorithm and FBA perform in the same way because they take the decision using the same criterion. The simulations for VA and FBA are run only up to 14 dB, with a length of $2^{23} - 2$, since the BER is below the required value of $10^{-5}$.

The gain in performance of the receivers by using the real channel, instead of the estimated one, is more than 1 dB for the same BER. This can be seen in Figure~\ref{fig:BER_real}. Note that in these simulations the VA and FBA are evaluated up to 13 dB. The lengths of the sequences of these simulation are in Table~\ref{table:BER_real_len}, and are greater than or equal to the ones in Table~\ref{table:BER_same_len} because for the same SNR $\Gamma$ the BER is lower.  

\begin{table}[h!]
	\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
	SNR $\Gamma$	&	5	&	6	&	7	&	8	&	9	&	10	&	11	&	12	&	13	&	14	&	15 \\ \hline	
	for LE 	(-2)	& $2^{13}$ & $2^{13}$ & $2^{13}$ & $2^{13}$ & $2^{14}$ & $2^{15}$ & $2^{15}$ & $2^{18}$ & $2^{18}$ & $2^{22}$ & $2^{22}$  \\
	for DFE (-2)	& $2^{13}$ & $2^{13}$ & $2^{13}$ & $2^{13}$ & $2^{15}$ & $2^{16}$ & $2^{18}$ & $2^{20}$ & $2^{22}$ & $2^{23}$ & $2^{23}$  \\
	for VA, FBA (-2)& $2^{15}$ & $2^{15}$ & $2^{15}$ & $2^{15}$ & $2^{18}$ & $2^{18}$ & $2^{20}$ & $2^{20}$ & $2^{22}$ & $2^{23}$ & $2^{23}$  \\
	\end{tabular}
	\caption{Length of sequences used to evaluate the BER, for same realization and estimate}
	\label{table:BER_same_len}
\end{table}

\begin{table}[h!]
	\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
	SNR $\Gamma$	&	5	&	6	&	7	&	8	&	9	&	10	&	11	&	12	&	13	&	14	&	15 \\ \hline	
	Every receiver (-2) & $2^{15}$ & $2^{15}$ & $2^{15}$ & $2^{15}$ & $2^{18}$ & $2^{18}$ & $2^{22}$ & $2^{23}$ & $2^{23}$ & $2^{23}$ & $2^{23}$  \\
	\end{tabular}
	\caption{Length of sequences used to evaluate the BER, assuming $\hat{h}_i = h_i$}
	\label{table:BER_real_len}
\end{table}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{BER_same_channel}
	\caption{Bit error rates for different methods, same realization and estimate}
	\label{fig:BER_same}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{BER_average}
	\caption{Bit error rates for different methods, averaged over different realizations}
	\label{fig:BER_avg}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{BER_real_channel}
	\caption{Bit error rates for various methods, assuming $\hat{h}_i = h_i$}
	\label{fig:BER_real}
\end{figure}

\clearpage

\begin{thebibliography}{10}

\bibitem{bc}
Benvenuto, Cherubini, Algorithms for Communications Systems and their Applications, Wiley, 2004

\end{thebibliography}

\end{document}
